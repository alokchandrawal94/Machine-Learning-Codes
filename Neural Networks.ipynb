{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcC32TEtFV4C"
   },
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SOZCSkukFsMZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from math import sqrt\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import Dense, Dropout\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZ1JpKheFRiy"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nc6OPe-OF8Nd"
   },
   "outputs": [],
   "source": [
    "filename1 = '/content/drive/My Drive/Machine Learning Homework 3/Q2_Test_Data.csv'\n",
    "filename2 = '/content/drive/My Drive/Machine Learning Homework 3/Q2_Train_Data.csv'\n",
    "filename3 = '/content/drive/My Drive/Machine Learning Homework 3/Q2_Validation_Data.csv'\n",
    "df_test = pd.read_csv(filename1)\n",
    "df_train = pd.read_csv(filename2)\n",
    "df_validation = pd.read_csv(filename3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYt3Gu7Lo4XP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "59uOWeIbGgXF",
    "outputId": "752a4f45-e1c1-4a59-a6f6-d4e17eed26f6"
   },
   "outputs": [],
   "source": [
    "print(df_test.shape)\n",
    "print(df_train.shape)\n",
    "print(df_validation.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "wPexpW20H9r9",
    "outputId": "023b55a4-96e3-4dd1-928a-6da499717ce9"
   },
   "outputs": [],
   "source": [
    "input_strings = df_train.pixels[50]\n",
    "arr = np.array(input_strings.split(), dtype=int)\n",
    "print(arr.shape)\n",
    "X = arr.reshape(48, 48)\n",
    "plt.imshow(X) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl3VRynCNSGY"
   },
   "source": [
    "# get two visualization per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_w4QQph3NIcv",
    "outputId": "a3ab642a-765f-4f45-b279-d3b42c7131be"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "emotions_dict = {0:'Angry', 1:'Disgust', 2:'Fear', 3:'Happy', 4:'Sad', 5:'Surprise', 6:'Neutral'}\n",
    "l = df_train.emotion.unique().size\n",
    "df_temp = pd.DataFrame()\n",
    "for emotion_number in range(0,l):\n",
    "  print('Emotion = ', emotions_dict[emotion_number].upper())\n",
    "  df_temp = df_train.loc[df_train['emotion']==emotion_number]\n",
    "  df_temp = df_temp.sample(n = 2)\n",
    "  indexes = list(df_temp.index)\n",
    "  for i in indexes:\n",
    "    input_strings = df_temp.pixels[i]\n",
    "    arr = np.array(input_strings.split(), dtype=int)\n",
    "    X = arr.reshape(48, 48)\n",
    "    plt.imshow(X) \n",
    "    plt.show() \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-gf4miCSd55"
   },
   "source": [
    "# Count the number of samples per emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "id": "Z77t1nz2SjH3",
    "outputId": "3c983ff2-f778-4d87-f75c-43880d842765"
   },
   "outputs": [],
   "source": [
    "for emotion_number in emotions_dict:\n",
    "  print('The number of sample for emotion {} is {}'.format(emotions_dict[emotion_number],df_train.loc[df_train['emotion']==emotion_number].shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgI8Z0vTV372"
   },
   "source": [
    "# you will use a feedforward neural network(FNN) (also called “multilayer perceptron”) to perform the emotion classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGdmJz_cVN2_"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "id": "bdkTmHUQgbur",
    "outputId": "c5efd555-f2ba-45b1-d1db-d4a5c98a91c1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#  Define a Feed-Forward Model with 2 hidden layers with dimensions 1152 and 576 Neurons\n",
    "model = Sequential([\n",
    "  Dense(2304, activation='relu', input_shape=(48*48,), name=\"first_hidden_layer\"),\n",
    "  Dense(2304//2, activation='relu', name=\"second_hidden_layer\"),Dropout(0.25),\n",
    "  Dense(7, activation='softmax'),\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3kx16M-Mw8-Y"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "def preprocess_image(test_list):\n",
    "  p = 0\n",
    "  for i in range(0, len(test_list)): \n",
    "    test_list[i] = float(test_list[i]) \n",
    "  list_to_return = []\n",
    "  mean = sum((test_list)) / len(test_list) \n",
    "  variance = sum([((x - mean) ** 2) for x in test_list]) / len(test_list) \n",
    "  res = variance ** 0.5\n",
    "  adj_std = max(res, 1.0/sqrt(len(test_list)))\n",
    "  for x in test_list:\n",
    "    p = (x - mean)/adj_std\n",
    "    list_to_return.append(p)\n",
    "  return list_to_return\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df_train):\n",
    "  #data preprocess\n",
    "  df_train['Array_pixels'] = df_train['pixels'].str.split(' ')\n",
    "  train_X = []\n",
    "  train_X1 = df_train['Array_pixels']\n",
    "  for l in train_X1:\n",
    "    list_final = preprocess_image(l)\n",
    "    train_X.append(list_final)\n",
    "  train_X = np.asarray(train_X).astype('float32')\n",
    "  train_Y = df_train['emotion']\n",
    "  train_Y = np.asarray(train_Y)\n",
    "  return train_X, train_Y\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ncy6yH8oEp5n"
   },
   "source": [
    "# training data accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "0AbS6MFpYauq",
    "outputId": "bbaef538-af7d-4559-d42b-0ef7fadeac13"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_X, train_Y = preprocess_data(df_train)\n",
    "start = time()\n",
    "performance = model.fit(train_X, to_categorical(train_Y), epochs=10, batch_size=256,)\n",
    "print('Training time for this model is = ', time()-start)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jo0Vle9hTWV2"
   },
   "source": [
    "# Validation data accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0FFkvuIjUfY6",
    "outputId": "183a249b-a41a-4d5d-e12c-7ef10272e73b"
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = preprocess_data(df_validation)\n",
    "performance_2 = model.evaluate(train_X, to_categorical(train_Y))\n",
    "\n",
    "# performance_2 = model.fit(train_X, to_categorical(train_Y), epochs=10, batch_size=256,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "eKJ3XDy_mUPo",
    "outputId": "f408ee4f-3c90-48a4-c36f-8f39be1df9f8"
   },
   "outputs": [],
   "source": [
    "train_X, train_Y = preprocess_data(df_test)\n",
    "performance_2 = model.evaluate(train_X, to_categorical(train_Y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNEPNoZCcY85"
   },
   "source": [
    "# plot loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "JErWY5QUovNN",
    "outputId": "5687a668-ec59-434f-f26c-a160dd0ecd8d"
   },
   "outputs": [],
   "source": [
    "def plot_loss_function(performance):\n",
    "  plt.plot(performance.history['loss'])\n",
    "  plt.title('model loss')\n",
    "  plt.ylabel('loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.legend(['train'], loc='upper left')\n",
    "  plt.show()\n",
    "\n",
    "plot_loss_function(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxqsFI0tcAm3"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itXfv3Gje5kt"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "def preprocess_image_cnn(test_list):\n",
    "  p = 0\n",
    "  for i in range(0, len(test_list)): \n",
    "    test_list[i] = float(test_list[i]) \n",
    "  list_to_return = []\n",
    "  mean = sum((test_list)) / len(test_list) \n",
    "  variance = sum([((x - mean) ** 2) for x in test_list]) / len(test_list) \n",
    "  res = variance ** 0.5\n",
    "  adj_std = max(res, 1.0/sqrt(len(test_list)))\n",
    "  for x in test_list:\n",
    "    p = (x - mean)/adj_std\n",
    "    list_to_return.append(p)\n",
    "  return list_to_return\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data_cnn(df_train):\n",
    "  #data preprocess\n",
    "  df_train['Array_pixels'] = df_train['pixels'].str.split(' ')\n",
    "  train_X = []\n",
    "  train_X1 = df_train['Array_pixels']\n",
    "  for l in train_X1:\n",
    "    list_final = preprocess_image(l)\n",
    "    train_X.append(list_final)\n",
    "  train_X = np.asarray(train_X).astype('float32')\n",
    "  train_Y = df_train['emotion']\n",
    "  train_Y = np.asarray(train_Y)\n",
    "  return train_X, train_Y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQMdv8l32gZm"
   },
   "outputs": [],
   "source": [
    "#get train, validation and test data\n",
    "train_X, train_Y = preprocess_data(df_train)\n",
    "train_X = train_X.reshape(28709,48,48,1)\n",
    "validation_X, validation_Y = preprocess_data(df_validation)\n",
    "validation_X = validation_X.reshape(3589,48,48,1)\n",
    "test_X, test_Y = preprocess_data(df_test)\n",
    "test_X = test_X.reshape(3589,48,48,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 938
    },
    "id": "NKizK9i7CIbg",
    "outputId": "3c9de338-6aec-4286-df74-edf3f244777f"
   },
   "outputs": [],
   "source": [
    "# we separate the feature layers from classifier layer to accomodate the later part of tutorial on Fine-tuning.\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "# Define 2 groups of layers: features layer (convolutions) and classification layer\n",
    "common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(48,48,1)),\n",
    "            Conv2D(32, kernel_size=3, activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "classifier = [Dense(512, activation='relu',kernel_regularizer=l2(0.01)), Dropout(0.25), Dense(7, activation='softmax'),]\n",
    "\n",
    "cnn_model = Sequential(common_features+classifier)\n",
    "\n",
    "print(cnn_model.summary())  # Compare number of parameteres against FFN\n",
    "cnn_model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "\n",
    "\n",
    "\n",
    "start = time()\n",
    "cnn_model.fit(train_X, to_categorical(train_Y), epochs=10, batch_size=256,)\n",
    "print('Training time for this model is = ', time()-start)\n",
    "\n",
    "# Acc\n",
    "performance = cnn_model.evaluate(validation_X, to_categorical(validation_Y))\n",
    "print(\"Accuracy on Validation samples: {0}\".format(performance[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "m5kNlScIu1qc",
    "outputId": "d7494838-7854-4c2c-f4d6-517d8d1c1a1a"
   },
   "outputs": [],
   "source": [
    "cnn_model.evaluate(test_X, to_categorical(test_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM7-jTnzWkQh"
   },
   "source": [
    "# Bayesian optimization for hyper-parameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2GzGl7pBVX-G",
    "outputId": "c47e5d38-d7cd-4df5-c735-038f6bff7156"
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "\n",
    "def optimize_cnn(hyperparameter):\n",
    "  \n",
    "  # Define model using hyperparameters \n",
    "  cnn_model = Sequential([Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu', input_shape=(48,48,1)), \n",
    "            Conv2D(32, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']),\n",
    "            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'),\n",
    "            Conv2D(64, kernel_size=hyperparameter['conv_kernel_size'], activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)), Dropout(hyperparameter['dropout_prob']), \n",
    "            Flatten(),\n",
    "            Dense(512, activation='relu', kernel_regularizer=l2(hyperparameter['lamda_value'])), \n",
    "            Dense(7, activation='softmax'),])\n",
    "  \n",
    "  cnn_model.compile(optimizer=hyperparameter['optimizer'], loss='categorical_crossentropy', metrics=['accuracy'],)\n",
    "\n",
    "  _ = cnn_model.fit(train_X, to_categorical(train_Y), epochs=2, batch_size=256, verbose=0)\n",
    "  # Evaluate accuracy on validation data\n",
    "  performance = cnn_model.evaluate(validation_X, to_categorical(validation_Y), verbose=0)\n",
    "\n",
    "  print(\"Hyperparameters: \", hyperparameter, \"Accuracy: \", performance[1])\n",
    "  print(\"----------------------------------------------------\")\n",
    "  # We want to minimize loss i.e. negative of accuracy\n",
    "  return({\"status\": STATUS_OK, \"loss\": -1*performance[1], \"model\":cnn_model})\n",
    "  \n",
    "\n",
    "# Define search space for hyper-parameters\n",
    "space = {\n",
    "    # The kernel_size for convolutions:\n",
    "    'conv_kernel_size': hp.choice('conv_kernel_size', [1, 3, 5]),\n",
    "    # Uniform distribution in finding appropriate dropout values\n",
    "    'dropout_prob': hp.uniform('dropout_prob', 0.1, 0.35),\n",
    "    # Choice of optimizer \n",
    "    'optimizer': hp.choice('optimizer', ['Adam', 'sgd']),\n",
    "     # Uniform distribution in finding appropriate regularization values\n",
    "    'lamda_value': hp.uniform('lamda_value', 0.01, 0.2),\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "# Find the best hyperparameters\n",
    "best = fmin(\n",
    "        optimize_cnn,\n",
    "        space,\n",
    "        algo=tpe.suggest,\n",
    "        trials=trials,\n",
    "        max_evals=50,\n",
    "    )\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Best Hyperparameters\", best)\n",
    "\n",
    "# You can retrain the final model with optimal hyperparameters on train+validation data\n",
    "\n",
    "# Or you can use the model returned directly\n",
    "# Find trial which has minimum loss value and use that model to perform evaluation on the test data\n",
    "test_model = trials.results[np.argmin([r['loss'] for r in trials.results])]['model']\n",
    "\n",
    "performance = test_model.evaluate(test_X, to_categorical(test_Y))\n",
    "\n",
    "print(\"==================================\")\n",
    "print(\"Test Accuracy: \", performance[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g55WucwEL-TU"
   },
   "source": [
    "# Fine tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 578
    },
    "id": "0R_zVk_rL_Iw",
    "outputId": "cd3ba433-ead0-4f0c-8515-4621cc189e5d"
   },
   "outputs": [],
   "source": [
    "common_features = [Conv2D(32, kernel_size=3, activation='relu', input_shape=(48,48,1)), \n",
    "            Conv2D(32, kernel_size=3, activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'),\n",
    "            Conv2D(64, kernel_size=3, activation='relu'), \n",
    "            MaxPooling2D(pool_size=(2,2)), Flatten(),]\n",
    "\n",
    "for l in common_features:\n",
    "  l.trainable = False\n",
    "\n",
    "classifier = [Dense(512, activation='relu',kernel_regularizer=l2(0.01)),Dropout(0.25), Dense(7, activation='softmax'),]\n",
    "cnn_model = Sequential(common_features+classifier)\n",
    "print(cnn_model.summary())\n",
    "cnn_model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "\n",
    "cnn_model.fit(validation_X, to_categorical(validation_Y), epochs=1, batch_size=256,)\n",
    "performance = cnn_model.evaluate(validation_X, to_categorical(validation_Y))\n",
    "# print(\"Accuracy on Validation samples: {0}\".format(performance[1]))\n",
    "performance = cnn_model.evaluate(test_X, to_categorical(test_Y))\n",
    "print(\"Accuracy on Test samples: {0}\".format(performance[1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZErJAaN1jDr"
   },
   "source": [
    "# Feature design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo7JE2S01qx7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "\n",
    "train_X,train_Y = preprocess_data(df_train)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGDWbq7x7Xd1"
   },
   "outputs": [],
   "source": [
    "train_X1 = []\n",
    "for image in train_X:\n",
    "  image = image.reshape(48,48)\n",
    "  fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1), visualize=True)\n",
    "  \n",
    "  train_X1.append(hog_image.reshape(2304))\n",
    "train_X1 = np.asarray(train_X1)\n",
    "print(train_X1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "4mM1ERaPQqp7",
    "outputId": "0cf9505e-ca4e-421f-bae5-a826bca3f261"
   },
   "outputs": [],
   "source": [
    "validation_X1 = []\n",
    "for image in validation_X:\n",
    "  image = image.reshape(48,48)\n",
    "  fd, hog_image = hog(image, orientations=8, pixels_per_cell=(16, 16),cells_per_block=(1, 1), visualize=True)\n",
    "  \n",
    "  validation_X1.append(hog_image.reshape(2304))\n",
    "validation_X1 = np.asarray(validation_X1)\n",
    "print(validation_X1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZhKjnWEVAIn"
   },
   "outputs": [],
   "source": [
    "validation_X, validation_Y = preprocess_data(df_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "Bi14fdtl7SG0",
    "outputId": "1b34f4c4-bbe3-43ff-fbd6-9cd32d330b32"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  Dense(2304, input_shape=(48*48,), name=\"Input_layer\"),\n",
    "  # Dense(2304//2, activation='relu', name=\"second_hidden_layer\"),Dropout(0.25),\n",
    "  Dense(7, activation='softmax'),\n",
    "])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy',metrics=['accuracy'],)\n",
    "start = time()\n",
    "performance = model.fit(train_X1, to_categorical(train_Y), epochs=10, batch_size=256,)\n",
    "print('Training time for this model is = ', time()-start)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "w8ouvh5MVJzW",
    "outputId": "68d9c716-2cb1-4d68-8808-16033603645f"
   },
   "outputs": [],
   "source": [
    "model.evaluate(validation_X1,to_categorical(validation_Y))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
